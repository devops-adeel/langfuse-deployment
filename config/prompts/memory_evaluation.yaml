# Memory Evaluation Prompt Templates
# LLM-as-Judge evaluators for memory quality assessment
# These are fallback templates used when Langfuse is unavailable

memory_relevance_judge:
  prompt: |
    Evaluate the relevance of retrieved memories to the query.

    Original Query: {{query}}
    Context: {{context}}

    Retrieved Memories:
    {{memories}}

    Evaluate each memory on:
    1. Direct relevance to query (0-1)
    2. Contextual appropriateness (0-1)
    3. Temporal relevance (0-1)
    4. Actionability (0-1)

    Overall Relevance Score: [0-1]

    Provide brief justification for scores.
  config:
    model: gpt-4o
    temperature: 0.1
    max_tokens: 1000

cross_domain_discovery_judge:
  prompt: |
    Assess the quality of cross-domain insights discovered.

    Query Domain: {{query_domain}}
    Target Domain: {{target_domain}}

    Discovered Insights:
    {{insights}}

    Evaluate:
    1. Connection validity (0-1)
    2. Insight novelty (0-1)
    3. Practical applicability (0-1)
    4. Bridge strength between domains (0-1)

    Discovery Quality Score: [0-1]

    Highlight most valuable connections.
  config:
    model: gpt-4o
    temperature: 0.2
    max_tokens: 1000

solution_effectiveness_judge:
  prompt: |
    Evaluate the effectiveness of a captured solution.

    Original Problem: {{problem}}
    Captured Solution: {{solution}}
    Actual Outcome: {{outcome}}

    Assess:
    1. Solution completeness (0-1)
    2. Clarity of steps (0-1)
    3. Reproducibility (0-1)
    4. Generalizability (0-1)
    5. Time efficiency (0-1)

    Effectiveness Score: [0-1]

    Identify missing elements or improvements.
  config:
    model: gpt-4o
    temperature: 0.1
    max_tokens: 1000

memory_freshness_judge:
  prompt: |
    Evaluate the freshness and continued relevance of a memory.

    Memory Created: {{created_date}}
    Last Accessed: {{last_accessed}}
    Domain: {{domain}}
    Content: {{content}}

    Current Context:
    - Technology versions: {{current_versions}}
    - Best practices: {{current_practices}}

    Evaluate:
    1. Technical accuracy (0-1)
    2. Version compatibility (0-1)
    3. Best practice alignment (0-1)
    4. Supersession need (0-1)

    Freshness Score: [0-1]

    Recommend: Keep as-is, Update, or Supersede
  config:
    model: gpt-4o
    temperature: 0.2
    max_tokens: 1000
